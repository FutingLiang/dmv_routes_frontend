import os
import sys
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, inspect, text
from datetime import datetime
import pytz
import re
import glob

# è¨­å®šæ§åˆ¶å°ç·¨ç¢¼ç‚ºUTF-8
if sys.platform == "win32":
    os.system('chcp 65001 > nul')
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# è¨­å®šè³‡æ–™å¤¾è·¯å¾‘
data_folder = r"C:\Users\root\Desktop\114å…¬è·¯ç¸½å±€_å®¢é‹è·¯ç·šè¡¨"
if not os.path.exists(data_folder):
    print("æ‰¾ä¸åˆ°æŒ‡å®šè³‡æ–™å¤¾ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢º")
    exit(1)

os.chdir(data_folder)

# PostgreSQL é€£ç·š
engine = create_engine('postgresql+psycopg2://postgres:s8304021@localhost:5432/postgres')

# å€åŸŸ / è·¯ç·šé¡å‹å°ç…§
district_map = {
    "è‡ºåŒ—å¸‚å€": "taipei", "è‡ºåŒ—": "taipei", "å°åŒ—": "taipei",
    "æ–°ç«¹": "hsinchu",
    "è‡ºä¸­": "taichung", "å°ä¸­": "taichung",
    "å˜‰ç¾©": "chiayi",
    "é«˜é›„": "kaohsiung",
}
route_map = {
    "åœ‹é“": "hwy_routes",
    "ä¸€èˆ¬å…¬è·¯": "local_routes",
    "ä¸€èˆ¬å®¢é‹": "local_routes",
    "ä¸€èˆ¬": "local_routes",
}

TARGET_TABLE = "dmv_routes_2025"
success_list, failed_list, skipped_list = [], [], []
table_created = False

tz = pytz.timezone("Asia/Taipei")
now_str = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S%z")

def clean_numeric_field(value):
    """æ¸…ç†æ•¸å€¼æ¬„ä½ï¼Œç§»é™¤éæ•¸å­—å­—ç¬¦ä¸¦è½‰æ›ç‚ºé©ç•¶é¡å‹"""
    if pd.isna(value) or value is None:
        return None
    
    # è½‰æ›ç‚ºå­—ä¸²ä¸¦æ¸…ç†
    str_val = str(value).strip()
    
    # ç§»é™¤æ›è¡Œç¬¦å’Œå¤šé¤˜ç©ºç™½
    str_val = re.sub(r'\s+', ' ', str_val)
    
    # å¦‚æœåŒ…å«éæ•¸å­—å­—ç¬¦ï¼ˆé™¤äº†å°æ•¸é»ï¼‰ï¼Œå˜—è©¦æå–æ•¸å­—éƒ¨åˆ†
    if re.search(r'[^\d\.]', str_val):
        # æå–ç¬¬ä¸€å€‹æ•¸å­—åºåˆ—
        match = re.search(r'(\d+\.?\d*)', str_val)
        if match:
            str_val = match.group(1)
        else:
            return None
    
    # å˜—è©¦è½‰æ›ç‚ºæ•¸å­—
    try:
        if '.' in str_val:
            return float(str_val)
        else:
            return int(str_val)
    except (ValueError, TypeError):
        return None

def clean_route_number(value):
    """æ¸…ç†è·¯ç·šç·¨è™Ÿï¼Œä¿æŒåŸå§‹æ ¼å¼ä½†ç¢ºä¿è³‡æ–™åº«ç›¸å®¹æ€§"""
    if pd.isna(value) or value is None:
        return None
    
    # è½‰æ›ç‚ºå­—ä¸²ä¸¦æ¸…ç†ç©ºç™½å­—ç¬¦
    str_val = str(value).strip()
    str_val = re.sub(r'\s+', ' ', str_val)
    
    return str_val if str_val else None

def normalize_column_names(df):
    """æ¨™æº–åŒ–æ¬„ä½åç¨±ï¼Œè™•ç†ä¸åŒæª”æ¡ˆçš„å‘½åå·®ç•°"""
    df_normalized = df.copy()
    
    # å»ºç«‹æ¬„ä½åç¨±å°æ‡‰è¡¨
    column_mapping = {
        # é‡Œç¨‹ç›¸é—œ
        'é‡Œ_ç¨‹': 'é‡Œç¨‹å¾€',
        'é‡Œç¨‹': 'é‡Œç¨‹å¾€',
        'é‡Œç¨‹_å¾€': 'é‡Œç¨‹å¾€',
        'é‡Œç¨‹_è¿”': 'é‡Œç¨‹è¿”',
        
        # ç­æ¬¡ç›¸é—œ
        'ç­_æ¬¡': 'ç­æ¬¡ä¸€',  # é€šç”¨ç­æ¬¡æ¬„ä½å°æ‡‰åˆ°ç­æ¬¡ä¸€
        'ç­_æ¬¡ä¸€': 'ç­æ¬¡ä¸€',
        'ç­_æ¬¡äºŒ': 'ç­æ¬¡äºŒ', 
        'ç­_æ¬¡ä¸‰': 'ç­æ¬¡ä¸‰',
        'ç­_æ¬¡å››': 'ç­æ¬¡å››',
        'ç­_æ¬¡äº”': 'ç­æ¬¡äº”',
        'ç­_æ¬¡å…­': 'ç­æ¬¡å…­',
        'ç­_æ¬¡æ—¥': 'ç­æ¬¡æ—¥',
        'ç­æ¬¡_ä¸€': 'ç­æ¬¡ä¸€',
        'ç­æ¬¡_äºŒ': 'ç­æ¬¡äºŒ',
        'ç­æ¬¡_ä¸‰': 'ç­æ¬¡ä¸‰',
        'ç­æ¬¡_å››': 'ç­æ¬¡å››',
        'ç­æ¬¡_äº”': 'ç­æ¬¡äº”',
        'ç­æ¬¡_å…­': 'ç­æ¬¡å…­',
        'ç­æ¬¡_æ—¥': 'ç­æ¬¡æ—¥',
        
        # è·¯ç·šæ€§è³ªç›¸é—œ
        'è·¯ç·šæ€§è³ª_(æ©Ÿå ´/ä¸€èˆ¬)': 'è·¯ç·šæ€§è³ª',
        'è·¯ç·šæ€§è³ª_æ©Ÿå ´_ä¸€èˆ¬': 'è·¯ç·šæ€§è³ª',
        'è·¯ç·šæ€§è³ª': 'è·¯ç·šæ€§è³ª',
        
        # å…¶ä»–æ¬„ä½
        'å…¬å¸_åç¨±': 'å…¬å¸åç¨±',
        'è·¯ç·š_ç·¨è™Ÿ': 'è·¯ç·šç·¨è™Ÿ',
        'è·¯ç·š_åç¨±': 'è·¯ç·šåç¨±',
        'è£œè²¼__è·¯ç·š': 'è£œè²¼_è·¯ç·š',
        'ç«™ç‰Œæ•¸': 'ç«™ç‰Œæ•¸å¾€',  # é€šç”¨ç«™ç‰Œæ•¸å°æ‡‰åˆ°ç«™ç‰Œæ•¸å¾€
        'ç«™ç‰Œæ•¸_å¾€': 'ç«™ç‰Œæ•¸å¾€',
        'ç«™ç‰Œæ•¸_è¿”': 'ç«™ç‰Œæ•¸è¿”',
        'è»Šè¼›_æ•¸': 'è»Šè¼›æ•¸',
        'è¯ç‡Ÿ_æ¥­è€…': 'è¯ç‡Ÿæ¥­è€…'
    }
    
    # é‡æ–°å‘½åæ¬„ä½
    df_normalized.rename(columns=column_mapping, inplace=True)
    
    return df_normalized

def clean_dataframe(df):
    """æ¸…ç†æ•´å€‹DataFrameçš„è³‡æ–™"""
    df_clean = df.copy()
    
    # æ•¸å€¼æ¬„ä½æ¸…ç†
    numeric_columns = ['é‡Œç¨‹å¾€', 'é‡Œç¨‹è¿”', 'ç­æ¬¡ä¸€', 'ç­æ¬¡äºŒ', 'ç­æ¬¡ä¸‰', 'ç­æ¬¡å››', 
                      'ç­æ¬¡äº”', 'ç­æ¬¡å…­', 'ç­æ¬¡æ—¥', 'ç«™ç‰Œæ•¸å¾€', 'ç«™ç‰Œæ•¸è¿”', 'è»Šè¼›æ•¸']
    
    for col in numeric_columns:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].apply(clean_numeric_field)
    
    # è·¯ç·šç·¨è™Ÿç‰¹æ®Šè™•ç†ï¼ˆä¿æŒç‚ºæ–‡å­—ä½†æ¸…ç†æ ¼å¼ï¼‰
    if 'è·¯ç·šç·¨è™Ÿ' in df_clean.columns:
        df_clean['è·¯ç·šç·¨è™Ÿ'] = df_clean['è·¯ç·šç·¨è™Ÿ'].apply(clean_route_number)
    
    # æ–‡å­—æ¬„ä½æ¸…ç†ï¼ˆç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œç¬¦ï¼‰
    text_columns = ['å…¬å¸åç¨±', 'è·¯ç·šåç¨±', 'è£œè²¼_è·¯ç·š', 'è¯ç‡Ÿæ¥­è€…', 'è·¯ç·šæ€§è³ª']
    for col in text_columns:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(str).apply(
                lambda x: re.sub(r'\s+', ' ', str(x).strip()) if pd.notna(x) and str(x).strip() != 'nan' else None
            )
    
    return df_clean

def pick_sheet_name(xlsx_path, preferred="å·¥ä½œè¡¨1"):
    try:
        xl = pd.ExcelFile(xlsx_path)
        if preferred in xl.sheet_names:
            return preferred
        return xl.sheet_names[0]
    except Exception:
        return preferred

def create_table_with_proper_types(df, table_name, engine):
    """å»ºç«‹å…·æœ‰é©ç•¶è³‡æ–™é¡å‹çš„è³‡æ–™è¡¨ï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¬„ä½"""
    
    # å®šç¾©å®Œæ•´çš„æ¬„ä½é¡å‹å°æ‡‰ï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½å‡ºç¾çš„æ¬„ä½ï¼‰
    column_types = {
        'å…¬å¸åç¨±': 'VARCHAR(100)',
        'è·¯ç·šç·¨è™Ÿ': 'VARCHAR(20)',
        'è·¯ç·šåç¨±': 'VARCHAR(200)',
        'é‡Œç¨‹å¾€': 'DECIMAL(10,2)',
        'é‡Œç¨‹è¿”': 'DECIMAL(10,2)',
        'ç­æ¬¡ä¸€': 'INTEGER',
        'ç­æ¬¡äºŒ': 'INTEGER',
        'ç­æ¬¡ä¸‰': 'INTEGER',
        'ç­æ¬¡å››': 'INTEGER',
        'ç­æ¬¡äº”': 'INTEGER',
        'ç­æ¬¡å…­': 'INTEGER',
        'ç­æ¬¡æ—¥': 'INTEGER',
        'è£œè²¼_è·¯ç·š': 'VARCHAR(10)',
        'ç«™ç‰Œæ•¸å¾€': 'INTEGER',
        'ç«™ç‰Œæ•¸è¿”': 'INTEGER',
        'è»Šè¼›æ•¸': 'INTEGER',
        'è¯ç‡Ÿæ¥­è€…': 'VARCHAR(200)',
        'è·¯ç·šæ€§è³ª': 'VARCHAR(20)',  # æ–°å¢è·¯ç·šæ€§è³ªæ¬„ä½
        'district': 'VARCHAR(20)',
        'route_type': 'VARCHAR(20)',
        'source_file': 'VARCHAR(200)',
        'imported_at': 'VARCHAR(30)'
    }
    
    # å»ºç«‹åŒ…å«æ‰€æœ‰å¯èƒ½æ¬„ä½çš„å®Œæ•´è³‡æ–™è¡¨
    columns_def = []
    for col_name, col_type in column_types.items():
        columns_def.append(f'"{col_name}" {col_type}')
    
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        {', '.join(columns_def)}
    )
    """
    
    with engine.connect() as conn:
        conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
        conn.execute(text(create_sql))
        conn.commit()

inspector = inspect(engine)

# ä½¿ç”¨globä¾†è™•ç†å¯èƒ½çš„ç·¨ç¢¼å•é¡Œ
xlsx_files = glob.glob("*.xlsx")
for file in xlsx_files:
    # æª¢æŸ¥æ˜¯å¦ç‚ºExcelè‡¨æ™‚æª”æ¡ˆï¼Œä½†ä»å˜—è©¦è™•ç†
    if file.startswith('~$'):
        print(f"âš ï¸ æª¢æ¸¬åˆ°Excelè‡¨æ™‚æª”æ¡ˆï¼Œå˜—è©¦è™•ç†ï¼š{file}")
        
    # æª¢æŸ¥æª”æ¡ˆåç¨±æ˜¯å¦åŒ…å«è·¯ç·šè³‡æ–™é—œéµå­—
    if "114" in file and ("è·¯ç·š" in file or "route" in file.lower()):
        file_clean = file.replace(" ", "")
        district_en = None
        route_type = None

        for zh, en in district_map.items():
            if zh in file_clean:
                district_en = en
                break
        for zh, en in route_map.items():
            if zh in file_clean:
                route_type = en
                break

        if not (district_en and route_type):
            try:
                print(f"âš ï¸ ç„¡æ³•è¾¨è­˜å€åŸŸæˆ–é¡å‹ï¼š{file}")
            except UnicodeEncodeError:
                print(f"âš ï¸ ç„¡æ³•è¾¨è­˜å€åŸŸæˆ–é¡å‹ï¼š{repr(file)}")
            skipped_list.append(file)
            continue

        try:
            sheet_name = pick_sheet_name(file, preferred="å·¥ä½œè¡¨1")
            df = pd.read_excel(file, sheet_name=sheet_name)

            # æ¬„åæ¸…ç†
            df.columns = (
                df.columns.astype(str)
                  .str.strip()
                  .str.replace(r"\s+", "_", regex=True)
                  .str.replace(r"[\r\n]+", "_", regex=True)
            )
            
            # ç§»é™¤ç©ºç™½æˆ–ç„¡åæ¬„ä½
            columns_to_drop = [col for col in df.columns if col.startswith('Unnamed:') or col.strip() == '']
            if columns_to_drop:
                df = df.drop(columns=columns_to_drop)
            
            # æ¨™æº–åŒ–æ¬„ä½åç¨±
            df = normalize_column_names(df)

            # è³‡æ–™æ¸…ç†
            df = clean_dataframe(df)

            # è¿½è¹¤æ¬„ä½
            df["district"]    = district_en
            df["route_type"]  = route_type
            df["source_file"] = file
            df["imported_at"] = now_str

            if not table_created:
                # å»ºç«‹å…·æœ‰é©ç•¶è³‡æ–™é¡å‹çš„è³‡æ–™è¡¨ï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½æ¬„ä½ï¼‰
                create_table_with_proper_types(df, TARGET_TABLE, engine)
                table_created = True
            
            # ç¢ºä¿DataFrameåŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½ï¼ˆå¡«å…¥Noneå¦‚æœä¸å­˜åœ¨ï¼‰
            required_columns = ['å…¬å¸åç¨±', 'è·¯ç·šç·¨è™Ÿ', 'è·¯ç·šåç¨±', 'é‡Œç¨‹å¾€', 'é‡Œç¨‹è¿”', 
                              'ç­æ¬¡ä¸€', 'ç­æ¬¡äºŒ', 'ç­æ¬¡ä¸‰', 'ç­æ¬¡å››', 'ç­æ¬¡äº”', 'ç­æ¬¡å…­', 'ç­æ¬¡æ—¥',
                              'è£œè²¼_è·¯ç·š', 'ç«™ç‰Œæ•¸å¾€', 'ç«™ç‰Œæ•¸è¿”', 'è»Šè¼›æ•¸', 'è¯ç‡Ÿæ¥­è€…', 'è·¯ç·šæ€§è³ª']
            
            for col in required_columns:
                if col not in df.columns:
                    df[col] = None

            # ä½¿ç”¨æ‰¹æ¬¡æ’å…¥ï¼Œä¸¦è™•ç†å¯èƒ½çš„è³‡æ–™é¡å‹å•é¡Œ
            try:
                df.to_sql(
                    TARGET_TABLE, engine,
                    if_exists="append", index=False,
                    chunksize=100, method="multi"
                )
            except Exception as insert_error:
                # å¦‚æœæ‰¹æ¬¡æ’å…¥å¤±æ•—ï¼Œå˜—è©¦é€è¡Œæ’å…¥ä»¥æ‰¾å‡ºå•é¡Œè³‡æ–™
                print(f"âš ï¸ æ‰¹æ¬¡æ’å…¥å¤±æ•—ï¼Œå˜—è©¦é€è¡Œæ’å…¥ï¼š{file}")
                successful_rows = 0
                
                for idx, row in df.iterrows():
                    try:
                        row_df = pd.DataFrame([row])
                        row_df.to_sql(
                            TARGET_TABLE, engine,
                            if_exists="append", index=False
                        )
                        successful_rows += 1
                    except Exception as row_error:
                        print(f"   ç¬¬ {idx+1} è¡Œæ’å…¥å¤±æ•—: {str(row_error)[:100]}...")
                        continue
                
                print(f"   æˆåŠŸæ’å…¥ {successful_rows}/{len(df)} è¡Œ")

            print(f"âœ… å·²åŒ¯å…¥ï¼š{file} â†’ {TARGET_TABLE}ï¼ˆdistrict={district_en}, route_type={route_type}ï¼‰")
            success_list.append((file, TARGET_TABLE))

        except Exception as e:
            error_msg = f"åŒ¯å…¥å¤±æ•—ï¼š{file}ï¼ŒéŒ¯èª¤ï¼š{str(e)}"
            print(f"âŒ {error_msg}")
            failed_list.append((file, str(e)))

# åŒ¯å…¥æ‘˜è¦èˆ‡å ±è¡¨
print("\nğŸ“‹ åŒ¯å…¥çµæœç¸½çµ")
print(f"âœ… æˆåŠŸåŒ¯å…¥ï¼š{len(success_list)} å€‹æª”æ¡ˆ â†’ {TARGET_TABLE}")
print(f"âŒ åŒ¯å…¥å¤±æ•—ï¼š{len(failed_list)} å€‹")
print(f"âš ï¸ ç•¥éæœªè­˜åˆ¥ï¼š{len(skipped_list)} å€‹")

# è¼¸å‡ºè©³ç´°å ±è¡¨
pd.DataFrame(success_list, columns=["æª”æ¡ˆåç¨±", "å¯«å…¥è³‡æ–™è¡¨"]).to_csv("åŒ¯å…¥æˆåŠŸæ¸…å–®.csv", index=False, encoding='utf-8-sig')
if failed_list:
    pd.DataFrame(failed_list, columns=["æª”æ¡ˆåç¨±", "éŒ¯èª¤è¨Šæ¯"]).to_csv("åŒ¯å…¥å¤±æ•—æ¸…å–®.csv", index=False, encoding='utf-8-sig')
if skipped_list:
    pd.DataFrame(skipped_list, columns=["æœªè­˜åˆ¥æª”æ¡ˆåç¨±"]).to_csv("ç•¥éæ¸…å–®.csv", index=False, encoding='utf-8-sig')

print("ğŸ“ å·²è¼¸å‡ºï¼šåŒ¯å…¥æˆåŠŸæ¸…å–®.csvã€åŒ¯å…¥å¤±æ•—æ¸…å–®.csvã€ç•¥éæ¸…å–®.csvï¼ˆå¦‚æœ‰ï¼‰")

# è³‡æ–™å“è³ªæª¢æŸ¥
try:
    with engine.connect() as conn:
        result = conn.execute(text(f"SELECT COUNT(*) as total_rows FROM {TARGET_TABLE}"))
        total_rows = result.fetchone()[0]
        
        result = conn.execute(text(f"""
            SELECT district, route_type, COUNT(*) as count 
            FROM {TARGET_TABLE} 
            GROUP BY district, route_type 
            ORDER BY district, route_type
        """))
        
        print(f"\nğŸ“Š è³‡æ–™çµ±è¨ˆï¼ˆç¸½è¨ˆ {total_rows} ç­†ï¼‰ï¼š")
        for row in result:
            print(f"   {row[0]} - {row[1]}: {row[2]} ç­†")
            
except Exception as e:
    print(f"âš ï¸ ç„¡æ³•åŸ·è¡Œè³‡æ–™çµ±è¨ˆï¼š{e}")
